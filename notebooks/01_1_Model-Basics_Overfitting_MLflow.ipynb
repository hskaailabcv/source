{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basics of Deep Learning Projects\n",
    "\n",
    "\n",
    "Subject of this notebook is to teach fundamental properties of deep learning projects. It is important being able to reproduce different runs of model training, as well as different runs should be comparable to each other. For this, we will use [MLflow](https://mlflow.org/), which is an open source platform to manage the ML lifecycle, including experimentation, reproducibility and deployment. It currently offers three components:\n",
    "\n",
    "* MLflow Tracking: Record and query experiments: code, data, config, and results.\n",
    "* MLflow Projects: Packaging format for reproducible runs on any platform.\n",
    "* MLflow Models: General format for sending models to diverse deployment tools.\n",
    "\n",
    "ToDo's:\n",
    "\n",
    "* Define different neural networks using `tf.keras`.\n",
    "* Train them and monitor the training using TensorBoard and MLflow\n",
    "* Try to intentionally overfit the training set. There are multiple ways to achieve this. \n",
    "* Compare different runs using MLflow. Can you spot the point where overfitting happened?\n",
    "\n",
    "\n",
    "Help:\n",
    "* [TensorFLow API Documentation](https://www.tensorflow.org/api_docs)\n",
    "* [MLflow Documentation](https://mlflow.org/docs/latest/index.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import mlflow\n",
    "import numpy as np\n",
    "\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script bash --bg\n",
    "\n",
    "mlflow server  \\\n",
    "--backend-store-uri \"postgresql://mlflow:mlflow@localhost:5432/mlflow\" \\\n",
    "--default-artifact-root file:./mlruns \\\n",
    "--host 0.0.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ToDo: load the dataset from the file `data/fashion_mnist.npz`. \n",
    "# The data accesible using the key `data`, the labels as `label` respectively.\n",
    "with np.load('data/fashion_mnist.npz') as fashion:\n",
    "    x = # ToDo\n",
    "    y = # ToDo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_split(x, y, at=0.8):\n",
    "    # ToDo: shuffle data and labels and perform train/test split at given fraction.\n",
    "    pass\n",
    "\n",
    "x_train, x_test, y_train, y_test = shuffle_split(x, y, at=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ToDo: normalize data\n",
    "x_train, x_test = # ToDo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print first 15 labels\n",
    "for i in range(15):\n",
    "    if i % 5 == 0:\n",
    "        print(\"\\n\")\n",
    "    print(y_train[i], end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plots the first 15 entries in the train set\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "for i in range(15):\n",
    "    plt.subplot(3, 5, i + 1)\n",
    "    plt.imshow(x_train[i], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ToDo: define input and output network parameters\n",
    "n_input =  # MNIST data input\n",
    "n_classes = # MNIST total classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encoding of labels\n",
    "def one_hot_encode(a, length):\n",
    "    temp = np.zeros((a.shape[0], length))\n",
    "    temp[np.arange(a.shape[0]), a] = 1\n",
    "    return temp\n",
    "\n",
    "y_train = one_hot_encode(y_train, n_classes)\n",
    "y_test = one_hot_encode(y_test, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: define hyper parameters\n",
    "training_iters = \n",
    "learning_rate = \n",
    "batch_size = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ToDo: MLP definition\n",
    "model = tf.keras.models.Sequential([\n",
    "    \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ToDo: define cost\n",
    "cost = \n",
    "# define optimizer\n",
    "optimizer = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: compile model with optimizer, loss function and validation metric\n",
    "model.compile(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ToDo: Train your model and evaluate on holdout test set\n",
    "import mlflow.tensorflow as mltf\n",
    "\n",
    "mltf.autolog(every_n_iter=1)\n",
    "\n",
    "model.fit(....)\n",
    "model.evaluate(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
