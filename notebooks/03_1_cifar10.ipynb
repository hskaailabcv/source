{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIFAR-10 Object Classification\n",
    "\n",
    "The CIFAR-10 dataset contains 60k 32x32 pixel color images from 10 different classes.\n",
    "\n",
    "The classes are:\n",
    "- airplane \n",
    "- automobile \n",
    "- bird \n",
    "- cat \n",
    "- deer \n",
    "- dog \n",
    "- frog \n",
    "- horse \n",
    "- ship \n",
    "- truck\n",
    "\n",
    "Tasks:\n",
    "\n",
    "- implement the TODOs\n",
    "- train a MLP to achieve >40% test accuracy\n",
    "- add TensorBoard summaries\n",
    "- train a CNN to achieve >80% test accuracy\n",
    "\n",
    "Help:\n",
    "- use the TensorFlow API Documentation [https://www.tensorflow.org/api_docs/](https://www.tensorflow.org/api_docs/)\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "# download CIFAR-10\n",
    "wget -q https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
    "# unpack\n",
    "tar xzf cifar-10-python.tar.gz\n",
    "# remove tar.gz\n",
    "rm cifar-10-python.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to unpickle data files\n",
    "def unpickle(file):\n",
    "    with open(file, 'rb') as fo:\n",
    "        obj = pickle.load(fo, encoding='bytes')\n",
    "    return obj\n",
    "\n",
    "# function to store  data in pickle file\n",
    "def store(obj, filename):\n",
    "    pickle.dump(obj, open('cifar-10-batches-py/' + filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test-code:\n",
    "```python\n",
    "x_train_b = unpickle('cifar-10-batches-py/data_batch_' + str(1)).get(bytes('data', 'ascii'))\n",
    "img = x_train_b[1]\n",
    "print(img.shape)\n",
    "r = img[0:1024]\n",
    "g = img[1024:2048]\n",
    "b = img[2048:3072]\n",
    "print(r.shape)\n",
    "print(g.shape)\n",
    "print(b.shape)\n",
    "rgb = np.dstack((r,g,b))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: decode pickle data as images\n",
    "# see https://www.cs.toronto.edu/~kriz/cifar.html\n",
    "def decode_as_image(img_flat):\n",
    "    img_R = img_flat[0:1024]\n",
    "    img_G = img_flat[1024:2048]\n",
    "    img_B = img_flat[2048:3072]\n",
    "    return np.dstack((img_R, img_G, img_B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load train data and save to disk for later usage\n",
    "# note: you might need to give Docker more memory\n",
    "# alternatively, execute separately\n",
    "x_train = []\n",
    "for i in range(1, 6):\n",
    "    x_train_b = unpickle('cifar-10-batches-py/data_batch_' + str(i)).get(bytes('data', 'ascii'))\n",
    "    for img in x_train_b:\n",
    "        img = decode_as_image(img)\n",
    "        x_train.append(img)\n",
    "\n",
    "# reshape the data\n",
    "x_train = np.array(x_train).reshape(5*10000, 32*32, 3)\n",
    "\n",
    "# save to disk\n",
    "store(x_train, 'x_train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load test data and save to disk for later usage\n",
    "x_test = []\n",
    "x_test_b = unpickle('cifar-10-batches-py/test_batch').get(bytes('data', 'ascii'))\n",
    "for img in x_test_b:\n",
    "    img = decode_as_image(img)\n",
    "    x_test.append(img)\n",
    "\n",
    "# reshape the data\n",
    "x_test = np.array(x_test).reshape(1*10000, 32*32, 3)\n",
    "\n",
    "# save to disk\n",
    "store(x_test, 'x_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load train labels and save to disk\n",
    "y_train = []\n",
    "for i in range(1, 6):\n",
    "    y_train_b = unpickle('cifar-10-batches-py/data_batch_' + str(i)).get(bytes('labels', 'ascii'))\n",
    "    for img in y_train_b:\n",
    "        y_train.append(img)\n",
    "        \n",
    "# reshape the data\n",
    "y_train = np.array(y_train).flatten()\n",
    "\n",
    "# save to disk\n",
    "store(y_train, 'y_train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load test labels and save to disk\n",
    "y_test = []\n",
    "y_test_b = unpickle('cifar-10-batches-py/test_batch').get(bytes('labels', 'ascii'))\n",
    "for img in y_test_b:\n",
    "    y_test.append(img)\n",
    "        \n",
    "# reshape the data\n",
    "y_test = np.array(y_test).flatten()\n",
    "\n",
    "# save to disk\n",
    "store(y_test, 'y_test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load prepared data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = unpickle(\"cifar-10-batches-py/x_train\")\n",
    "x_test = unpickle(\"cifar-10-batches-py/x_test\")\n",
    "y_train = unpickle(\"cifar-10-batches-py/y_train\")\n",
    "y_test = unpickle(\"cifar-10-batches-py/y_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapping from label number to label\n",
    "label_mapping = ['airplane','automobile','bird','cat','deer','dog','frog','horse','ship','truck']\n",
    "\n",
    "def get_label(i):\n",
    "    return label_mapping[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plots the first 3 entries in the train set\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "rand = np.random.randint(50000 - 1)\n",
    "i = 0\n",
    "for idx in range(rand, rand + 3):\n",
    "    plt.subplot(1, 3, i + 1)\n",
    "    plt.title(\"Class: {}\".format(get_label(int(y_train[idx]))))\n",
    "    plt.imshow(x_train[idx].reshape(32,32,3))\n",
    "    i += 1\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.reshape(-1, 32*32*3)\n",
    "x_test = x_test.reshape(-1, 32*32*3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 1024, 3)\n"
     ]
    }
   ],
   "source": [
    "# TODO: normalize data and cast to float32\n",
    "x_train = x_train.astype(np.float32) / 255.0\n",
    "x_test = x_test.astype(np.float32) / 255.0\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 32, 32, 3)\n",
      "(10000, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "x_train = np.reshape(x_train, (-1, 32,32,3))\n",
    "x_test = np.reshape(x_test, (-1, 32, 32, 3))\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: define network parameters\n",
    "n_input = 3072 # image shape\n",
    "n_channels = 3 # number of channels\n",
    "n_classes = 10 # number of CIFAR-10 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6 9 9 ... 9 1 1]\n"
     ]
    }
   ],
   "source": [
    "# one hot encoding of labels\n",
    "def one_hot_encode(a, length):\n",
    "    temp = np.zeros((a.shape[0], length))\n",
    "    temp[np.arange(a.shape[0]), a] = 1\n",
    "    return temp\n",
    "\n",
    "print(y_train)\n",
    "y_train = one_hot_encode(y_train.astype(np.int), n_classes)\n",
    "y_test = one_hot_encode(y_test, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: define placeholder\n",
    "x = tf.placeholder(tf.float32, [None, n_input], \"x\")\n",
    "y = tf.placeholder(tf.float32, [None, n_classes], \"y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: define hyper parameters\n",
    "learning_rate = 0.0001\n",
    "training_iters = 1000000\n",
    "batch_size = 256\n",
    "display_step = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp(x):\n",
    "    # TODO: define MLP\n",
    "    flatten = tf.layers.flatten(x)\n",
    "    hidden1 = tf.layers.dense(inputs=flatten, units=255, activation=tf.nn.relu)\n",
    "    dropout = tf.layers.dropout(inputs=hidden1, rate=0.4)\n",
    "    hidden2 = tf.layers.dense(inputs=dropout, units=255, activation=tf.nn.relu)\n",
    "    pred = tf.layers.dense(hidden2, 10, activation=tf.nn.softmax)\n",
    "    \n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.reset_default_graph()\n",
    "#x = tf.placeholder(tf.float32, shape=(None, 32, 32, 3), name='input_x')\n",
    "#y = tf.placeholder(tf.float32, shape=(None, 10), name='output_y')\n",
    "x = tf.placeholder(tf.float32, shape=(None, 32, 32, 3), name='input_x')\n",
    "def cnn(x):\n",
    "    weight_decay = 1e-4\n",
    "    \n",
    "    x = tf.reshape(x, [-1, 3, 32,32])\n",
    "    x = tf.transpose(x, perm=[0,2,3,1])\n",
    "    \n",
    "    conv1 = tf.layers.conv2d(inputs=x, filters=32, kernel_size=(3,3), padding='SAME', activation=tf.nn.elu, \n",
    "                             kernel_regularizer=tf.contrib.layers.l2_regularizer(weight_decay))\n",
    "    bn1 = tf.layers.batch_normalization(conv1)\n",
    "    conv2 = tf.layers.conv2d(inputs=bn1, filters=32, kernel_size=(3,3), padding='SAME', activation=tf.nn.elu, \n",
    "                        kernel_regularizer=tf.contrib.layers.l2_regularizer(weight_decay))\n",
    "    bn2 = tf.layers.batch_normalization(conv2)\n",
    "    maxp1 = tf.layers.max_pooling2d(inputs=bn2, pool_size=(2,2), strides=(2,2))\n",
    "    dropout1 = tf.layers.dropout(inputs=maxp1, rate=0.2)\n",
    "\n",
    "    conv3 = tf.layers.conv2d(inputs=dropout1, filters=64, kernel_size=(3,3), padding='SAME', activation=tf.nn.elu,\n",
    "                            kernel_regularizer=tf.contrib.layers.l2_regularizer(weight_decay))\n",
    "    bn3 = tf.layers.batch_normalization(conv3)\n",
    "    conv4 = tf.layers.conv2d(inputs=bn3, filters=64, kernel_size=(3,3), padding='SAME', activation=tf.nn.elu,\n",
    "                            kernel_regularizer=tf.contrib.layers.l2_regularizer(weight_decay))\n",
    "    bn4 = tf.layers.batch_normalization(conv4)\n",
    "    maxp2 = tf.layers.max_pooling2d(inputs=bn4, pool_size=(2,2), strides=(2,2))\n",
    "    dropout2 = tf.layers.dropout(inputs=maxp2, rate=0.3)\n",
    "    \n",
    "    conv5 = tf.layers.conv2d(inputs=dropout2, filters=128, kernel_size=(3,3), padding='SAME', activation=tf.nn.elu,\n",
    "                            kernel_regularizer=tf.contrib.layers.l2_regularizer(weight_decay))\n",
    "    bn5 = tf.layers.batch_normalization(conv5)\n",
    "    conv6 = tf.layers.conv2d(inputs=bn5, filters=128, kernel_size=(3,3), padding='SAME', activation=tf.nn.elu,\n",
    "                            kernel_regularizer=tf.contrib.layers.l2_regularizer(weight_decay))\n",
    "    bn6 = tf.layers.batch_normalization(conv6)\n",
    "    maxp3 = tf.layers.max_pooling2d(inputs=bn6, pool_size=(2,2), strides=(2,2))\n",
    "    dropout3 = tf.layers.dropout(inputs=maxp3, rate=0.4)\n",
    "    \n",
    "    flatten = tf.layers.flatten(inputs=dropout3)\n",
    "    dense = tf.layers.dense(inputs=flatten, units=n_classes, activation=tf.nn.softmax)\n",
    "    \n",
    "    return dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build MLP / CNN Network from class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build network\n",
    "#pred = mlp(x)\n",
    "pred = cnn(x)\n",
    "\n",
    "# define cost function and optimizer\n",
    "cost = tf.reduce_mean(tf.losses.softmax_cross_entropy(y, pred))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# evaluate model\n",
    "correct_pred = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# TODO: define tensorboard summaries\n",
    "#ith tf.name_scope('performance'):\n",
    "#   tf_loss_ph = tf.placeholder(tf.float32,shape=None,name='loss_summary')\n",
    "#   tf.summary.scalar('loss', tf_loss_ph)\n",
    "#  tf_accuracy_ph = tf.placeholder(tf.float32,shape=None, name='accuracy_summary')\n",
    "#   tf_accuracy_summary = tf.summary.scalar('accuracy', tf_accuracy_ph)\n",
    "\n",
    "train_acc = tf.summary.scalar('accuracy', accuracy)\n",
    "train_cost = tf.summary.scalar('cost', cost)\n",
    "\n",
    "merged = tf.summary.merge_all() # merges train acc and cost summaries\n",
    "\n",
    "test_acc = tf.summary.scalar('accuracy_test', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and evaluate MLP / CNN from Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 32, 32, 3)\n",
      "(128, 32, 32, 3)\n",
      "(128, 32, 32, 3)\n",
      "(128, 32, 32, 3)\n",
      "(128, 32, 32, 3)\n",
      "(128, 32, 32, 3)\n",
      "(128, 32, 32, 3)\n",
      "(128, 32, 32, 3)\n",
      "(128, 32, 32, 3)\n",
      "(128, 32, 32, 3)\n",
      "(128, 32, 32, 3)\n",
      "(128, 32, 32, 3)\n",
      "(128, 32, 32, 3)\n",
      "(128, 32, 32, 3)\n",
      "(128, 32, 32, 3)\n",
      "(128, 32, 32, 3)\n",
      "(128, 32, 32, 3)\n",
      "(128, 32, 32, 3)\n",
      "(128, 32, 32, 3)\n",
      "(128, 32, 32, 3)\n",
      "(128, 32, 32, 3)\n",
      "(128, 32, 32, 3)\n",
      "(128, 32, 32, 3)\n",
      "(128, 32, 32, 3)\n",
      "(128, 32, 32, 3)\n",
      "(128, 32, 32, 3)\n",
      "(128, 32, 32, 3)\n",
      "(128, 32, 32, 3)\n",
      "(128, 32, 32, 3)\n",
      "(128, 32, 32, 3)\n",
      "(128, 32, 32, 3)\n",
      "(128, 32, 32, 3)\n",
      "(128, 32, 32, 3)\n",
      "(128, 32, 32, 3)\n",
      "(128, 32, 32, 3)\n",
      "(128, 32, 32, 3)\n",
      "(128, 32, 32, 3)\n",
      "(128, 32, 32, 3)\n",
      "(128, 32, 32, 3)\n",
      "(128, 32, 32, 3)\n",
      "(128, 32, 32, 3)\n",
      "(128, 32, 32, 3)\n",
      "(128, 32, 32, 3)\n",
      "(128, 32, 32, 3)\n",
      "(128, 32, 32, 3)\n",
      "(128, 32, 32, 3)\n",
      "(128, 32, 32, 3)\n",
      "(128, 32, 32, 3)\n",
      "(128, 32, 32, 3)\n",
      "(128, 32, 32, 3)\n",
      "(128, 32, 32, 3)\n",
      "(128, 32, 32, 3)\n",
      "(128, 32, 32, 3)\n",
      "(128, 32, 32, 3)\n",
      "(128, 32, 32, 3)\n",
      "(128, 32, 32, 3)\n",
      "(128, 32, 32, 3)\n",
      "(128, 32, 32, 3)\n",
      "(128, 32, 32, 3)\n",
      "(128, 32, 32, 3)\n",
      "(128, 32, 32, 3)\n",
      "(128, 32, 32, 3)\n",
      "(128, 32, 32, 3)\n",
      "(128, 32, 32, 3)\n",
      "(128, 32, 32, 3)\n",
      "(128, 32, 32, 3)\n",
      "(128, 32, 32, 3)\n",
      "(128, 32, 32, 3)\n",
      "(128, 32, 32, 3)\n",
      "(128, 32, 32, 3)\n",
      "(128, 32, 32, 3)\n",
      "(128, 32, 32, 3)\n",
      "(128, 32, 32, 3)\n",
      "(128, 32, 32, 3)\n",
      "(128, 32, 32, 3)\n",
      "(128, 32, 32, 3)\n",
      "(128, 32, 32, 3)\n",
      "(128, 32, 32, 3)\n",
      "(128, 32, 32, 3)\n",
      "(128, 32, 32, 3)\n",
      "(128, 32, 32, 3)\n",
      "(128, 32, 32, 3)\n",
      "(128, 32, 32, 3)\n",
      "(128, 32, 32, 3)\n",
      "(128, 32, 32, 3)\n",
      "(128, 32, 32, 3)\n",
      "(128, 32, 32, 3)\n",
      "(128, 32, 32, 3)\n",
      "(128, 32, 32, 3)\n",
      "(128, 32, 32, 3)\n",
      "(128, 32, 32, 3)\n",
      "(128, 32, 32, 3)\n",
      "(128, 32, 32, 3)\n",
      "(128, 32, 32, 3)\n",
      "(128, 32, 32, 3)\n",
      "(128, 32, 32, 3)\n",
      "(128, 32, 32, 3)\n",
      "(128, 32, 32, 3)\n",
      "(128, 32, 32, 3)\n",
      "(128, 32, 32, 3)\n",
      "Step: 100\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "You must feed a value for placeholder tensor 'x_1' with dtype float and shape [?,3072]\n\t [[node x_1 (defined at <ipython-input-45-934852e91faa>:2) ]]\n\t [[node Mean (defined at <ipython-input-18-516e5d85b7b7>:6) ]]\n\nCaused by op 'x_1', defined at:\n  File \"/usr/lib/python3.5/runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python3.5/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelapp.py\", line 505, in start\n    self.io_loop.start()\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/platform/asyncio.py\", line 148, in start\n    self.asyncio_loop.run_forever()\n  File \"/usr/lib/python3.5/asyncio/base_events.py\", line 345, in run_forever\n    self._run_once()\n  File \"/usr/lib/python3.5/asyncio/base_events.py\", line 1312, in _run_once\n    handle._run()\n  File \"/usr/lib/python3.5/asyncio/events.py\", line 125, in _run\n    self._callback(*self._args)\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/ioloop.py\", line 690, in <lambda>\n    lambda f: self._run_callback(functools.partial(callback, future))\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/ioloop.py\", line 743, in _run_callback\n    ret = callback()\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/gen.py\", line 781, in inner\n    self.run()\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/gen.py\", line 742, in run\n    yielded = self.gen.send(value)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 357, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 267, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 534, in execute_request\n    user_expressions, allow_stdin,\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/ipkernel.py\", line 294, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2843, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2869, in _run_cell\n    return runner(coro)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/async_helpers.py\", line 67, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 3044, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 3209, in run_ast_nodes\n    if (yield from self.run_code(code, result)):\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 3291, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-45-934852e91faa>\", line 2, in <module>\n    x = tf.placeholder(tf.float32, [None, n_input], \"x\")\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/array_ops.py\", line 2077, in placeholder\n    return gen_array_ops.placeholder(dtype=dtype, shape=shape, name=name)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gen_array_ops.py\", line 5791, in placeholder\n    \"Placeholder\", dtype=dtype, shape=shape, name=name)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py\", line 788, in _apply_op_helper\n    op_def=op_def)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 3300, in create_op\n    op_def=op_def)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 1801, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nInvalidArgumentError (see above for traceback): You must feed a value for placeholder tensor 'x_1' with dtype float and shape [?,3072]\n\t [[node x_1 (defined at <ipython-input-45-934852e91faa>:2) ]]\n\t [[node Mean (defined at <ipython-input-18-516e5d85b7b7>:6) ]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: You must feed a value for placeholder tensor 'x_1' with dtype float and shape [?,3072]\n\t [[{{node x_1}}]]\n\t [[{{node Mean}}]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-3cc2e20a507c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Step:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0;31m# calculate train batch loss and accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msummary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmerged\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m             \u001b[0;31m#loss, acc = sess.run([cost, accuracy], feed_dict={x: batch_x, y: batch_y})\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0mtrain_writer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1346\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1347\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merror_interpolation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1348\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1350\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: You must feed a value for placeholder tensor 'x_1' with dtype float and shape [?,3072]\n\t [[node x_1 (defined at <ipython-input-45-934852e91faa>:2) ]]\n\t [[node Mean (defined at <ipython-input-18-516e5d85b7b7>:6) ]]\n\nCaused by op 'x_1', defined at:\n  File \"/usr/lib/python3.5/runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python3.5/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelapp.py\", line 505, in start\n    self.io_loop.start()\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/platform/asyncio.py\", line 148, in start\n    self.asyncio_loop.run_forever()\n  File \"/usr/lib/python3.5/asyncio/base_events.py\", line 345, in run_forever\n    self._run_once()\n  File \"/usr/lib/python3.5/asyncio/base_events.py\", line 1312, in _run_once\n    handle._run()\n  File \"/usr/lib/python3.5/asyncio/events.py\", line 125, in _run\n    self._callback(*self._args)\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/ioloop.py\", line 690, in <lambda>\n    lambda f: self._run_callback(functools.partial(callback, future))\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/ioloop.py\", line 743, in _run_callback\n    ret = callback()\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/gen.py\", line 781, in inner\n    self.run()\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/gen.py\", line 742, in run\n    yielded = self.gen.send(value)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 357, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 267, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 534, in execute_request\n    user_expressions, allow_stdin,\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/ipkernel.py\", line 294, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2843, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2869, in _run_cell\n    return runner(coro)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/async_helpers.py\", line 67, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 3044, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 3209, in run_ast_nodes\n    if (yield from self.run_code(code, result)):\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 3291, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-45-934852e91faa>\", line 2, in <module>\n    x = tf.placeholder(tf.float32, [None, n_input], \"x\")\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/array_ops.py\", line 2077, in placeholder\n    return gen_array_ops.placeholder(dtype=dtype, shape=shape, name=name)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gen_array_ops.py\", line 5791, in placeholder\n    \"Placeholder\", dtype=dtype, shape=shape, name=name)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py\", line 788, in _apply_op_helper\n    op_def=op_def)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 3300, in create_op\n    op_def=op_def)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 1801, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nInvalidArgumentError (see above for traceback): You must feed a value for placeholder tensor 'x_1' with dtype float and shape [?,3072]\n\t [[node x_1 (defined at <ipython-input-45-934852e91faa>:2) ]]\n\t [[node Mean (defined at <ipython-input-18-516e5d85b7b7>:6) ]]\n"
     ]
    }
   ],
   "source": [
    "# initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "progbar = tf.keras.utils.Progbar(training_iters, stateful_metrics=[\"loss\", \"acc\"])\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    train_writer = tf.summary.FileWriter('tf-summary/train', sess.graph)\n",
    "    test_writer = tf.summary.FileWriter('tf-summary/test')\n",
    "    step = 1\n",
    "    \n",
    "    # training loop\n",
    "    while step * batch_size < training_iters:\n",
    "        indices = np.random.randint(x_train.shape[0], size=batch_size)\n",
    "        batch_x = x_train[indices]\n",
    "        print(batch_x.shape)\n",
    "        batch_y = y_train[indices]\n",
    "        # run optimization op (backprop)\n",
    "        if step % display_step == 0:\n",
    "            print(\"Step:\", step)\n",
    "            # calculate train batch loss and accuracy\n",
    "            loss, acc, summary = sess.run([cost, accuracy, merged], feed_dict={x: batch_x, y: batch_y})\n",
    "            #loss, acc = sess.run([cost, accuracy], feed_dict={x: batch_x, y: batch_y})\n",
    "            train_writer.add_summary(summary, step*batch_size)\n",
    "            \n",
    "            progbar.update(step*batch_size, values=[(\"loss\", loss), (\"acc\", acc)])\n",
    "            \n",
    "            # TODO: calculate test accuracy of random test batch\n",
    "            indices = np.random.randint(x_test.shape[0], size=batch_size)\n",
    "            test_batch_x = x_test[indices]\n",
    "            test_batch_y = y_test[indices]\n",
    "            acc = sess.run(test_acc, feed_dict={x: test_batch_x, y: test_batch_y})\n",
    "            test_writer.add_summary(acc, step*batch_size)\n",
    "        else:\n",
    "            #print(\"Step:\", step)\n",
    "            sess.run(optimizer, feed_dict={x: batch_x, y: batch_y})\n",
    "        step += 1\n",
    "    \n",
    "    print(\"\\n\")\n",
    "    print (\"Optimization Finished!\")\n",
    "    \n",
    "    # calculate accuracy for MNIST test images\n",
    "    print (\"Testing Accuracy:\", \\\n",
    "        sess.run(accuracy, feed_dict={x: x_test,\n",
    "                                      y: y_test}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN from [this](https://towardsdatascience.com/cifar-10-image-classification-in-tensorflow-5b501f7dc77c) tutorial from \"TowardsDataScience\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper parameters for the CNN\n",
    "epochs = 20\n",
    "batch_size = 128\n",
    "keep_probability = 0.5\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "x = tf.placeholder(tf.float32, shape=(None, 32, 32, 3), name='input_x')\n",
    "y =  tf.placeholder(tf.float32, shape=(None, 10), name='output_y')\n",
    "\n",
    "def cnn(x):\n",
    "    # TODO: define CNN\n",
    "    \n",
    "    #keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "    \n",
    "    conv1_filter = tf.Variable(tf.truncated_normal(shape=[3, 3, 3, 64], mean=0, stddev=0.08))\n",
    "    conv2_filter = tf.Variable(tf.truncated_normal(shape=[3, 3, 64, 128], mean=0, stddev=0.08))\n",
    "    conv3_filter = tf.Variable(tf.truncated_normal(shape=[5, 5, 128, 256], mean=0, stddev=0.08))\n",
    "    conv4_filter = tf.Variable(tf.truncated_normal(shape=[5, 5, 256, 512], mean=0, stddev=0.08))\n",
    "\n",
    "    # 1, 2\n",
    "    conv1 = tf.nn.conv2d(x, conv1_filter, strides=[1,1,1,1], padding='SAME')\n",
    "    conv1 = tf.nn.relu(conv1)\n",
    "    conv1_pool = tf.nn.max_pool(conv1, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
    "    conv1_bn = tf.layers.batch_normalization(conv1_pool)\n",
    "\n",
    "    # 3, 4\n",
    "    conv2 = tf.nn.conv2d(conv1_bn, conv2_filter, strides=[1,1,1,1], padding='SAME')\n",
    "    conv2 = tf.nn.relu(conv2)\n",
    "    conv2_pool = tf.nn.max_pool(conv2, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')    \n",
    "    conv2_bn = tf.layers.batch_normalization(conv2_pool)\n",
    "  \n",
    "    # 5, 6\n",
    "    conv3 = tf.nn.conv2d(conv2_bn, conv3_filter, strides=[1,1,1,1], padding='SAME')\n",
    "    conv3 = tf.nn.relu(conv3)\n",
    "    conv3_pool = tf.nn.max_pool(conv3, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')  \n",
    "    conv3_bn = tf.layers.batch_normalization(conv3_pool)\n",
    "    \n",
    "    # 7, 8\n",
    "    conv4 = tf.nn.conv2d(conv3_bn, conv4_filter, strides=[1,1,1,1], padding='SAME')\n",
    "    conv4 = tf.nn.relu(conv4)\n",
    "    conv4_pool = tf.nn.max_pool(conv4, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
    "    conv4_bn = tf.layers.batch_normalization(conv4_pool)\n",
    "    \n",
    "    # 9\n",
    "    flat = tf.contrib.layers.flatten(conv4_bn)  \n",
    "\n",
    "    # 10\n",
    "    full1 = tf.contrib.layers.fully_connected(inputs=flat, num_outputs=128, activation_fn=tf.nn.relu)\n",
    "    full1 = tf.nn.dropout(full1, 1 - keep_probability)\n",
    "    full1 = tf.layers.batch_normalization(full1)\n",
    "    \n",
    "    # 11\n",
    "    full2 = tf.contrib.layers.fully_connected(inputs=full1, num_outputs=256, activation_fn=tf.nn.relu)\n",
    "    full2 = tf.nn.dropout(full2, 1 - keep_probability)\n",
    "    full2 = tf.layers.batch_normalization(full2)\n",
    "    \n",
    "    # 12\n",
    "    full3 = tf.contrib.layers.fully_connected(inputs=full2, num_outputs=512, activation_fn=tf.nn.relu)\n",
    "    full3 = tf.nn.dropout(full3, 1 - keep_probability)\n",
    "    full3 = tf.layers.batch_normalization(full3)    \n",
    "    \n",
    "    # 13\n",
    "    full4 = tf.contrib.layers.fully_connected(inputs=full3, num_outputs=1024, activation_fn=tf.nn.relu)\n",
    "    full4 = tf.nn.dropout(full4, 1 - keep_probability)\n",
    "    full4 = tf.layers.batch_normalization(full4)        \n",
    "    \n",
    "    # 14\n",
    "    pred = tf.contrib.layers.fully_connected(inputs=full3, num_outputs=10, activation_fn=None)\n",
    "    \n",
    "    #x = tf.reshape(x, [-1, 3, 32,32]).transpose(0,2,3,1)\n",
    "    #conv1 = tf.layers.conv2d(inputs=x, filters=32, kernel_size=[5,5], data_format=\"channels_last\", activation=tf.nn.relu)\n",
    "    #pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=(3,3), strides=1)\n",
    "    #flatten = tf.layers.flatten(inputs=pool1)\n",
    "    #pred = tf.layers.dense(flatten, 10, activation=tf.nn.softmax)\n",
    "    #pred = tf.layers.dense(x, 10, activation=tf.nn.softmax)\n",
    "    \n",
    "    return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build CNN Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'logits' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-93ac20be0bfb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Loss and Optimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mcost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax_cross_entropy_with_logits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdamOptimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcost\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'logits' is not defined"
     ]
    }
   ],
   "source": [
    "pred = cnn(x)\n",
    "logits = cnn(x)\n",
    "model = tf.identity(logits, name='logits') # Name logits Tensor, so that can be loaded from disk after training\n",
    "\n",
    "# Loss and Optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "\n",
    "# Tensorboard summaries\n",
    "train_acc = tf.summary.scalar('accuracy', accuracy)\n",
    "train_cost = tf.summary.scalar('cost', cost)\n",
    "\n",
    "merged = tf.summary.merge_all() # merges train acc and cost summaries\n",
    "\n",
    "test_acc = tf.summary.scalar('accuracy_test', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Evaluate CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 998400/1000000 [============================>.] - ETA: 0s - loss: 2.3025 - acc: 0.1094\n",
      "\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.1\n"
     ]
    }
   ],
   "source": [
    "# initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "progbar = tf.keras.utils.Progbar(training_iters, stateful_metrics=[\"loss\", \"acc\"])\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    train_writer = tf.summary.FileWriter('tf-summary/train', sess.graph)\n",
    "    test_writer = tf.summary.FileWriter('tf-summary/test')\n",
    "    step = 1\n",
    "    \n",
    "    # training loop\n",
    "    while step * batch_size < training_iters:\n",
    "        indices = np.random.randint(x_train.shape[0], size=batch_size)\n",
    "        batch_x = x_train[indices]\n",
    "        batch_x = batch_x.reshape((len(batch_x), 3, 32, 32)).transpose(0, 2, 3, 1)\n",
    "        batch_y = y_train[indices]\n",
    "        # run optimization op (backprop)\n",
    "        if step % display_step == 0:\n",
    "            #print(\"Step:\", step)\n",
    "            # calculate train batch loss and accuracy\n",
    "            loss, acc, summary = sess.run([cost, accuracy, merged], feed_dict={x: batch_x, y: batch_y})\n",
    "            #loss, acc = sess.run([cost, accuracy], feed_dict={x: batch_x, y: batch_y})\n",
    "            train_writer.add_summary(summary, step*batch_size)\n",
    "            \n",
    "            progbar.update(step*batch_size, values=[(\"loss\", loss), (\"acc\", acc)])\n",
    "            \n",
    "            # TODO: calculate test accuracy of random test batch\n",
    "            indices = np.random.randint(x_test.shape[0], size=batch_size)\n",
    "            test_batch_x = x_test[indices]\n",
    "            test_batch_x = test_batch_x.reshape((len(test_batch_x), 3, 32, 32)).transpose(0, 2, 3, 1)\n",
    "            test_batch_y = y_test[indices]\n",
    "            acc = sess.run(test_acc, feed_dict={x: test_batch_x, y: test_batch_y})\n",
    "            test_writer.add_summary(acc, step*batch_size)\n",
    "        else:\n",
    "            #print(\"Step:\", step)\n",
    "            sess.run(optimizer, feed_dict={x: batch_x, y: batch_y})\n",
    "        step += 1\n",
    "    \n",
    "    print(\"\\n\")\n",
    "    print (\"Optimization Finished!\")\n",
    "    \n",
    "    # calculate accuracy for MNIST test images\n",
    "    x_test_reshaped = x_test.reshape((len(x_test), 3, 32, 32)).transpose(0, 2, 3, 1)\n",
    "    print (\"Testing Accuracy:\", \\\n",
    "        sess.run(accuracy, feed_dict={x: x_test_reshaped,\n",
    "                                      y: y_test}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch['data'].reshape((len(batch['data']), 3, 32, 32)).transpose(0, 2, 3, 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
