{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIFAR-10 Object Classification\n",
    "\n",
    "The CIFAR-10 dataset contains 60k 32x32 pixel color images from 10 different classes.\n",
    "\n",
    "The classes are:\n",
    "- airplane \n",
    "- automobile \n",
    "- bird \n",
    "- cat \n",
    "- deer \n",
    "- dog \n",
    "- frog \n",
    "- horse \n",
    "- ship \n",
    "- truck\n",
    "\n",
    "Tasks:\n",
    "\n",
    "- implement the TODOs\n",
    "- train a MLP to achieve >40% test accuracy\n",
    "- add TensorBoard summaries\n",
    "- train a CNN to achieve >80% test accuracy\n",
    "\n",
    "Help:\n",
    "- use the TensorFlow API Documentation [https://www.tensorflow.org/api_docs/](https://www.tensorflow.org/api_docs/)\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "# download CIFAR-10\n",
    "wget -q https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
    "# unpack\n",
    "tar xzf cifar-10-python.tar.gz\n",
    "# remove tar.gz\n",
    "rm cifar-10-python.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to unpickle data files\n",
    "def unpickle(file):\n",
    "    with open(file, 'rb') as fo:\n",
    "        obj = pickle.load(fo, encoding='bytes')\n",
    "    return obj\n",
    "\n",
    "# function to store  data in pickle file\n",
    "def store(obj, filename):\n",
    "    pickle.dump(obj, open('cifar-10-batches-py/' + filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: decode pickle data as images\n",
    "# see https://www.cs.toronto.edu/~kriz/cifar.html\n",
    "def decode_as_image(img_flat):\n",
    "    img_R = \n",
    "    img_G = \n",
    "    img_B = \n",
    "    return np.dstack((img_R, img_G, img_B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load train data and save to disk for later usage\n",
    "# note: you might need to give Docker more memory\n",
    "# alternatively, execute separately\n",
    "x_train = []\n",
    "for i in range(1, 6):\n",
    "    x_train_b = unpickle('cifar-10-batches-py/data_batch_' + str(i)).get(bytes('data', 'ascii'))\n",
    "    for img in x_train_b:\n",
    "        img = decode_as_image(x_train_b)\n",
    "        x_train.append(img)\n",
    "\n",
    "# reshape the data\n",
    "x_train = np.array(x_train).reshape(5*10000, 32*32, 3)\n",
    "\n",
    "# save to disk\n",
    "store(x_train, 'x_train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load test data and save to disk for later usage\n",
    "x_test = []\n",
    "x_test_b = unpickle('cifar-10-batches-py/test_batch').get(bytes('data', 'ascii'))\n",
    "for img in x_test_b:\n",
    "    img = decode_as_image(x_train_b)\n",
    "    x_test.append(img)\n",
    "\n",
    "# reshape the data\n",
    "x_test = np.array(x_test).reshape(1*10000, 32*32, 3)\n",
    "\n",
    "# save to disk\n",
    "store(x_test, 'x_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load train labels and save to disk\n",
    "y_train = []\n",
    "for i in range(1, 6):\n",
    "    y_train_b = unpickle('cifar-10-batches-py/data_batch_' + str(i)).get(bytes('labels', 'ascii'))\n",
    "    for img in y_train_b:\n",
    "        y_train.append(img)\n",
    "        \n",
    "# reshape the data\n",
    "y_train = np.array(y_train).flatten()\n",
    "\n",
    "# save to disk\n",
    "store(y_train, 'y_train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load test labels and save to disk\n",
    "y_test = []\n",
    "y_test_b = unpickle('cifar-10-batches-py/test_batch').get(bytes('labels', 'ascii'))\n",
    "for img in y_test_b:\n",
    "    y_test.append(img)\n",
    "        \n",
    "# reshape the data\n",
    "y_test = np.array(y_test).flatten()\n",
    "\n",
    "# save to disk\n",
    "store(y_test, 'y_test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load prepared data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = unpickle(\"cifar-10-batches-py/x_train\")\n",
    "x_test = unpickle(\"cifar-10-batches-py/x_test\")\n",
    "y_train = unpickle(\"cifar-10-batches-py/y_train\")\n",
    "y_test = unpickle(\"cifar-10-batches-py/y_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapping from label number to label\n",
    "label_mapping = ['airplane','automobile','bird','cat','deer','dog','frog','horse','ship','truck']\n",
    "\n",
    "def get_label(i):\n",
    "    return label_mapping[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plots the first 3 entries in the train set\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "rand = np.random.randint(50000 - 1)\n",
    "i = 0\n",
    "for idx in range(rand, rand + 3):\n",
    "    plt.subplot(1, 3, i + 1)\n",
    "    plt.title(\"Class: {}\".format(get_label(int(y_train[idx]))))\n",
    "    plt.imshow(x_train[idx].reshape(32,32,3))\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.reshape(-1, 32*32*3)\n",
    "x_test = x_test.reshape(-1, 32*32*3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: normalize data and cast to float32\n",
    "x_train, x_test = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: define network parameters\n",
    "n_input =  # image shape\n",
    "n_channels =  # number of channels\n",
    "n_classes =  # number of CIFAR-10 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encoding of labels\n",
    "def one_hot_encode(a, length):\n",
    "    temp = np.zeros((a.shape[0], length))\n",
    "    temp[np.arange(a.shape[0]), a] = 1\n",
    "    return temp\n",
    "\n",
    "y_train = one_hot_encode(y_train, n_classes)\n",
    "y_test = one_hot_encode(y_test, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: define placeholder\n",
    "x = \n",
    "y = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: define hyper parameters\n",
    "learning_rate =\n",
    "training_iters = \n",
    "batch_size = \n",
    "display_step = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp(x):\n",
    "    # TODO: define MLP\n",
    "\n",
    "    pred = tf.layers.dense(x, 10, activation=tf.nn.softmax)\n",
    "    \n",
    "    return pred\n",
    "\n",
    "def cnn(x):\n",
    "    # TODO: define CNN\n",
    "\n",
    "    pred = tf.layers.dense(x, 10, activation=tf.nn.softmax)\n",
    "\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build network\n",
    "pred = mlp(x)\n",
    "#pred = cnn(input)\n",
    "\n",
    "# define cost function and optimizer\n",
    "cost = tf.reduce_mean(tf.losses.softmax_cross_entropy(y, pred))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# evaluate model\n",
    "correct_pred = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# TODO: define tensorboard summaries\n",
    "train_acc = \n",
    "train_cost = \n",
    "\n",
    "merged = tf.summary.merge_all() # merges train acc and cost summaries\n",
    "\n",
    "test_acc = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "progbar = tf.keras.utils.Progbar(training_iters, stateful_metrics=[\"loss\", \"acc\"])\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    train_writer = tf.summary.FileWriter('tf-summary/train', sess.graph)\n",
    "    test_writer = tf.summary.FileWriter('tf-summary/test')\n",
    "    step = 1\n",
    "    \n",
    "    # training loop\n",
    "    while step * batch_size < training_iters:\n",
    "        indices = np.random.randint(x_train.shape[0], size=batch_size)\n",
    "        batch_x = x_train[indices]\n",
    "        batch_y = y_train[indices]\n",
    "        # run optimization op (backprop)\n",
    "        if step % display_step == 0:\n",
    "            # calculate train batch loss and accuracy\n",
    "            loss, acc, summary = sess.run([cost, accuracy, merged], feed_dict={x: batch_x, y: batch_y})\n",
    "            train_writer.add_summary(summary, step*batch_size)\n",
    "            \n",
    "            progbar.update(step*batch_size, values=[(\"loss\", loss), (\"acc\", acc)])\n",
    "            \n",
    "            # TODO: calculate test accuracy of random test batch\n",
    "            test_batch_x = \n",
    "            test_batch_y = \n",
    "            acc = \n",
    "            test_writer.add_summary(acc, step*batch_size)\n",
    "        else:\n",
    "            sess.run(optimizer, feed_dict={x: batch_x, y: batch_y})\n",
    "        step += 1\n",
    "    \n",
    "    print(\"\\n\")\n",
    "    print (\"Optimization Finished!\")\n",
    "    \n",
    "    # calculate accuracy for MNIST test images\n",
    "    print (\"Testing Accuracy:\", \\\n",
    "        sess.run(accuracy, feed_dict={x: x_test,\n",
    "                                      y: y_test}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
